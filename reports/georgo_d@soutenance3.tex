
%%% PARTIE DE DIMMII


\chapter{Notre reseau de neurones} % (fold)
\label{cha:notre_reseau_de_neurones}

\section{Introduction} % (fold)
\label{subsec:introduction}
A l'issue de la premi\`ere soutenance, nos recherches sur les r\'eseaux
neuronaux nous avaient men\'es vers l'\'elaboration d'un perceptron
multicouches, celui-ci \'etant reconnu comme la star de la
reconnaissance de caract\`eres.

Le temps a pass\'e', \'ecrivait G\'erard de Narval, pensant \`a sa jeunesse
et toutes ces belles ann\'ees d\'esormais r\'evolues... En ce qui
concerne le r\'eseau de neurones, le temps a \'egalement pass\'e et
notre perceptron multicouches commence \`a prendre de l'allure. Sa
jeunesse n'\'etant pas encore r\'evolue, grand bien lui fasse, il a
encore beaucoup \`a apprendre... Alors
certes il ne reconna\^it pas encore grand chose, mais une premi\`ere
\'ebauche de sa structure est pr\^ete et les premiers tests ont \'et\'e
effectu\'es sur les donn\'ees d'un XOR et celles d'un afficheur sept
segments. La phase d'apprentissage reste \`a mettre au point car la
version actuelle ne donne pas les r\'esultats attendus. Mais ne parlons
pas de malheur, et rentrons d\`es \`a pr\'esent dans le vif du sujet : what about
multi-layer perceptron ?! 

Nous d\'ecrirons ici le perceptron tel que nous l'avons abord\'e, c'est \`a
dire \`a un stade encore exp\'erimental. Pour motiver sa conception,
nous nous sommes plac\'es dans le cas o\`u notre r\'eseau devra
reconna\^itre les caract\`eres provenant d'un afficheur sept
segments. En effet, les donn\'ees d'un tel alphabet sont assez simples
\`a formater sans pour autant atteindre le niveau de simplicit\'e d'une
fonction logique de base comme le OR ou l\'eg\`erement plus avanc\'ee
comme le XOR

% subsection introduction (end)
\pagebreak
\section{Structure et d\'efinitions} % (fold)
\label{subsec:structure_et_definitions}

Voici ce \`a quoi ressemble la b\^ete :
\begin{center}
	
	\includegraphics[width=120mm]{ra-perceptron.png}\\
	\caption{\emph{Perceptron \`a une couche cach\'ee}}\\
\end{center}

%pour ce paragraphe j'aimerais une liste à puce à la place de mes étoiles
%=)
Un perceptron est un ensemble de neurones reli\'es par des connexions
pond\'er\'ees. Ce n'est rien d'autre qu'un graphe non-orient\'e valu\'e
en somme, du moins en apparence. On y rep\`ere trois ensembles
particuliers de neurones que sont : 
%debut de liste à puce
*la couche d'entr\'ee : elle poss\`ede un certain nombre de neurones qui
correspondent \`aux entr\'ees du r\'eseau. La mod\'elisation et le
formatage des donn\'ees d'entr\'ee sont donc primordiaux si l'on veut
\'eviter d'avoir un trop grand nombre de neurones, ce qui augmenterait
le temps de la reconnaissance par la suite.

*la ou les couches cach\'ee(s) : ce sont ces neurones qui permettront de
reconna\^itre des donn\'ees. Le nombre de couches cach\'ees et de
neurones par couche est \`a adapter selon le probl\`eme et en
particulier selon la taille de l'\'echantillon de donn\'ees que l'on veut
apprendre au r\'eseau.

*la couche de sortie : ce sont ces neurones qui contiendront le
r\'esultat de la reconnaissance. Il y en a, a priori, autant que
d'\'elements \`a reconna\^itre, mais une seule sortie ensuite filtr\'ee
par une table de hachage,par exemple, reste envisageable.
%fin de liste a puce

% Mettre cette photo ici !

\begin{center}
	
	\includegraphics[width=70mm]{sortie008.png}\\
	\caption{\emph{Perceptron \`a une sortie}}\\
\end{center}




% avec la légende suivante : Perceptron \`a une sortie

La valeur d'un neurone est calcul\'ee \`a partir des valeurs de chacun
des neurones de la couche pr\'ec\'edente.

Les neurones artificiels ainsi \'evoqu\'es sont en fait tr\`es proches
des neurones naturels au niveau de leur fonctionnement. C'est entre
autres l'id\'ee d'int\'eractions avec une multitude d'autres neurones
qui permet au r\'eseau de se constituer une ``m\'emoire intelligente'',
apte \`a classifier ou reconna\^itre des donn\'ees qu'il n'a encore
``jamais'' rencontr\'ees auparavant (du moins pas exactement sous la
même forme) !

Avant de pouvoir utiliser un perceptron multicouches, il faut avant tout se mettre d'accord sur le
format de donn\'ees qu'il prendra en entr\'ee, puis effectuer un choix
judicieux de son nombre de couches cach\'ees et de son nombre de neurones
par couche cach\'ee. On doit ensuite ``entra\^iner'' le r\'eseau\ldots

% subsection structure_et_definitions (end)

\section{Formatage des donn\'ees} % (fold)
\label{subsec:formatage_des_donnees}

Pour que le r\'eseau reconnaisse, dans notre cas, si un caract\`ere est
telle ou telle lettre, il faut affecter une valeur pr\'ecise à chacun de
ses neurones d'entr\'e.
Parmi les id\'ees qui nous sont venues, il y a l'extraction de
caract\'eristiques des caract\`eres que l'on a isol\'e \`a partir du document
scann\'e. Ainsi l'on pourrait envisager de vectoriser chaque caract\`ere
afin d'en extraire des segments caract\'eristiques que l'on donnerait en
entr\'ee au r\'eseau. On pourrait aussi imaginer effectuer une premi\`ere
classification par crit\`ere de largeur, puis de hauteur, etc, en utilisant
plusieurs r\'eseaux en s\'erie par exemple, jusqu'\`a d\'etermination totale du
caract\`ere.

La m\'ethode que nous avons retenue \`a ce jour est n\'eanmoins bien
plus simpliste, et pas n\'ecessairement moins efficace : notre
perceptron recevra directement la matrice de pixels associ\'ee au
caract\`ere \`a reconna\^itre, apr\`es calibrage et centrage de chaque
caract\`ere dans une matrice de taille fixe.
Chaque neurone d'entr\'ee du r\'eseau recevra donc l'un des pixels de la
matrice de caract\`ere (d'o\`u la n\'ecessit\'e de r\'eduire au maximum
la taille et l'\'epaisseur de nos caract\`eres, afin de ne pas
d\'epasser la centaine de neurones d'entr\'ee).
Les donn\'ees d'apprentissage seront quant \`a elles pr\'esent\'ees sous
forme de couples de vecteurs d'entr\'ees/sorties au r\'eseau et
stock\'ees dans un fichier.


% subsection formatage_des_donnees (end)

\section{Param\`etres du r\'eseau} % (fold)
\label{subsec:choix_des_parametres_du_reseau}

Une fois les donn\'ees format\'ees et stock\'ees dans un fichier, il
faut choisir un certain nombre d'éléments clés du réseau.

Nous avons donc choisi d'affecter chaque pixel de la matrice du
caract\`ere \`a reconna\^itre \`a un neurone d'entr\'ee.

Dans le mod\`ele retenu actuellement, nous pr\'evoyons aussi d'affecter
un neurone de sortie par caract\`ere \`a reconna\^itre. Le caract\`ere
reconnu sera celui dont la valeur sera \`a un alors que les autres seront
rest\'es \`a z\'ero.

Pour ce qui est des couches cach\'ees, il est recommand\'e dans le cas
de la reconnaissance de caract\`eres d'en prendre entre une et deux, et
de choisir un nombre de neurones par couche du m\^eme ordre de grandeur
que le nombre de neurones de la couche de sortie. Mais nous essayerons
plusieurs configurations jusqu'\`a obtenir des r\'esultats
performants.

Un certain nombres de param\`etres relatifs \`a l'apprentissage des
donn\'ees par le r\'eseau doivent \'egalement \^etre fix\'es. Nous en
ferons une br\`eve description dans la section ``Entra\^inement du r\'eseau.''

% subsection choix_des_parametres_du_reseau (end)

\section{Entra\^inement du r\'eseau} % (fold)
\label{subsec:entrainement_du_reseau}

Cette partie est tr\`es probablement la plus int\'eressante de
l'impl\'ementation du perceptron multicouches. C'est en effet ici que
l'on doit mettre en oeuvre le fonctionnement du r\'eseau \`a proprement
parler. 

Son principe est le suivant :

Dans un premier temps, on affecte des petites valeurs \`a tous les poids
des connexions neuronales (entre 0,5 et -0,5 dans notre cas).

Ensuite, pour un nombre d'it\'erations donn\'e par exemple, ou bien tant
que l'erreur totale du vecteur de sorties est supérieure à un seuil fixé, on effectue
les \'etapes suivantes :

On permute l'ordre des donn\'ees que l'on pr\'esente au r\'eseau, ceci
d'uniformiser l'apprentissage, de sorte \`a ce qu'il n'apprenne pas \`a reconna\^itre toujours la m\^eme
s\'equence de donn\'ees.

On fournit ensuite un couple d'entr\'ees/sorties d\'esir\'ees au
r\'eseau.

Il faut maintenant propager vers l'avant les valeurs des neurones
d'entr\'ee, de sorte \`a obtenir les valeurs de sortie.

Pour cela on utilise une fonction de combinaison, ici une somme
pond\'er\'ee sum, dans notre cas
d\'efinie comme, pour chaque neurone ni, la somme des produits des valeurs
des neurones nj qui pointent sur lui par les poids des connexions pij qui les
lient au neurone courant :

\[sum(ni) = somme\, sur\, j\, (pij(ni,nj)* valeur(nj))
\]% une formule en LaTeX serait plus jolie ici
\\

\begin{center}
	
	\includegraphics[width=70mm]{neuron.jpg}\\
	\caption{\emph{Calcul de la sortie d'un neurone}}\\
\end{center}




Cette valeur est ensuite pass\'ee en param\`etre d'une fonction
d'activation propre au perceptron, ici la fonction sigmo\"ide f avec :  

\[f(sum(ni)) = 1/(1+exp(-sum(ni))) 
\]% une formule en LaTeX serait plus jolie ici

On obtient ainsi une valeur exp\'erimentale pour chaque neurone de
sortie que l'on va pouvoir confronter \`a la valeur th\'eorique fournie
initialement.

On calcule donc l'erreur e observ\'ee pour chaque neurone de sortie ni avec :

\[
	e(ni) = (valeur\, theorique(ni)\, -\, valeur\, observee(ni))*(f'(sum(ni)))
\]

% une formule en LaTeX serait plus jolie ici

On propage ensuite l'erreur vers les couches intérieures 

Puis on modifie les poids de toutes les connexions du réseau.

Et voilà, notre réseau marche !! Du moins pour apprendre le XOR !! Car
nous avons hélas manqué de temps pour conclure ce merveilleux projet et
nous comptons bien sur les vacances d'été pour le recoder intégralement
!



% subsection entrainement_du_reseau (end)

\subsection{Le banc de tests du perceptron} % (fold)
\label{subsec:le_banc_de_tests_du_perceptron}

Un petit shell a \'et\'e impl\'ement\'e afin de tester notre perceptron
au fur et \`a mesure de sa cr\'eation. Il est lanc\'e par d\'efaut lors
de l'ex\'ecution du programme ``network'' et propose diverses
options. 
%une grosse liste à puce please !
\begin{itemize}
	\item Trois fonctions d'affichage sont disponibles : 
\begin{description}
	\item[\verb!dispc! :] affiche l'\'etat des connexions neuronales et leur nombre
	\item[\verb!dispn! :] affiche l'\'etat des neurones et leur nombre
	\item[\verb!dispo! :] affiche les valeurs du vecteur de sorties (outputs)
\end{description}
%fin de liste, merci !

\item On peut initialiser le r\'eseau avec la fonction init, qui demandera
successivement de renseigner le nombre de couches cach\'ees d\'esir\'ees
ainsi que le nombre de neurones par couche cach\'ee.

\item La fonction train entra\^ine le r\'eseau (pour l'instant assez mal, il
faut l'admettre) 

\item La fonction active effectue une simple propagation des valeurs
d'entr\'ee vers celles de sortie

\item La fonction run permet (Bertrand) quant \`a elle, de pr\'eciser l'indice de la donn\'ee
que l'on souhaite tester (donn\'ee pr\'esente dans data.ml ; la donn\'ee
par d\'efaut est celle correspondant au XOR, il suffit de swaper les
noms de datatab et datatab2 dans data.ml pour acc\'eder au vecteur de
donn\'ees de l'afficheur sept segments !)

\item Enfin une fonction load et une fonction save sont pr\'esentes pour une
impl\'ementation future proche !? Prévu pour le mois d'août donc !

\end{itemize}

%fin de grosse liste à puce danke schön !

Un jour, ce shell de test sera am\'elior\'e et compl\'et\'e afin de rendre les
tests de notre r\'eseau rapides et efficaces. Il n'est pas destin\'e \`a
l'utilisateur final et c'est pourquoi nous avons pris quelques
libert\'es quant \`a son nombre de ligne de code.
On pourra bient\^ot pr\'eciser l'ensemble des param\`etres et
coefficients du r\'eseau via le shell, ce qui nous permettra
d'identifier clairement les options que nous t\^acherons,
,\'eventuellement dans une autre vie, de linker \`a une interface graphique
sp\'ecifique au r\'eseau qui sera enti\`erement param\'etrable et donc
r\'eutilisable \`a part enti`ere pour d'autres projets.

La HUGE team, c'est avant tout une team. Ce n'est que dans un second
temps que l'on se rend compte \`a quelle point, en plus d'\^etre une
team, c'est une team HUGE !!!  
%subsection le_banc_de_tests_du_perceptron (end)

% section notre_reseau_de_neurones (end)


%% fin du chapitre sur les reseaux de neuronnes
%%
%% + Reprise du cahier des charges,
%% + Plusieurs presentations possibles :
%%      - Chronologique (groupe),
%%      - Chronologique (individuelle),
%%      - Individuelles (repartition des taches),
%%      - Autres.
%% + Recit de la realisation :
%%      - Ses joies,
%%      - Ses peines,
%%      - Etc.
%% + Les annexes comprennent :
%%      - Les exemples d'impression,
%%      - les exemples d'       cran,
%%      - les jeux d'essai,
%%      - les dessins d'origine,
%%      - Etc.t
